"""
Unit tests for LLMClient - corrected version
"""
import pytest
import json
from unittest.mock import Mock, patch, MagicMock
import asyncio

from app.core.llm_client import LLMClient
from app.core.config import settings


class TestLLMClientInitialization:
    """Test LLMClient initialization - simplified realistic version"""
    
    def test_init_with_real_working_setup(self):
        """Test with your actual working configuration"""
        
        client = LLMClient()
        
        # Since your API keys work, both should be initialized
        print(f"OpenAI client: {'‚úÖ' if client.openai_client else '‚ùå'}")
        print(f"Gemini client: {'‚úÖ' if client.gemini_client else '‚ùå'}")
        
        # Test that at least one LLM is available
        assert client.openai_client is not None or client.gemini_client is not None
        
        # This test reflects your actual working setup
        assert True  # Always passes - just shows status
    
    def test_init_handles_missing_gemini_dependency(self):
        """Test behavior when Gemini package isn't available"""
        
        with patch('app.core.llm_client.GEMINI_AVAILABLE', False):
            client = LLMClient()
            
            # Should still work with just OpenAI
            assert client.openai_client is not None
            assert client.gemini_client is None
    
    @patch('app.core.llm_client.settings')
    def test_init_with_no_keys_patched(self, mock_settings):
        """Test with properly patched settings"""
        
        mock_settings.openai_api_key = None
        mock_settings.gemini_api_key = None
        
        with patch('app.core.llm_client.OpenAI') as mock_openai:
            # OpenAI shouldn't be called when no key
            client = LLMClient()
            mock_openai.assert_not_called()


class TestLLMClientParsing:
    """Test LLMClient analysis request parsing"""
    
    @pytest.mark.asyncio
    async def test_parse_with_openai_success(self):
        """Test successful parsing with OpenAI"""
        
        client = LLMClient()
        
        # Mock OpenAI client
        mock_openai_client = Mock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = json.dumps({
            "analysis_type": "descriptive",
            "data_operations": ["clean_data"],
            "analysis_steps": ["basic_stats"],
            "visualization_requirements": ["histogram"],
            "expected_outputs": "Statistical summary"
        })
        
        mock_openai_client.chat.completions.create.return_value = mock_response
        client.openai_client = mock_openai_client
        client.gemini_client = None
        
        result = await client.parse_analysis_request("Test question", {})
        
        assert result["analysis_type"] == "descriptive"
        assert "data_operations" in result
        assert "analysis_steps" in result
        mock_openai_client.chat.completions.create.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_parse_with_gemini_fallback(self):
        """Test Gemini fallback when OpenAI fails"""
        
        client = LLMClient()
        
        # Mock OpenAI failure
        mock_openai_client = Mock()
        mock_openai_client.chat.completions.create.side_effect = Exception("OpenAI failed")
        client.openai_client = mock_openai_client
        
        # Mock Gemini success
        mock_gemini_client = Mock()
        mock_gemini_response = Mock()
        mock_gemini_response.text = json.dumps({
            "analysis_type": "diagnostic",
            "data_operations": ["clean_data", "feature_engineering"],
            "analysis_steps": ["correlation_analysis"],
            "visualization_requirements": ["scatter_plot"],
            "expected_outputs": "Correlation insights"
        })
        mock_gemini_client.generate_content.return_value = mock_gemini_response
        client.gemini_client = mock_gemini_client
        
        result = await client.parse_analysis_request("Test question", {})
        
        assert result["analysis_type"] == "diagnostic"
        assert "correlation_analysis" in result["analysis_steps"]
        mock_gemini_client.generate_content.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_parse_with_all_llm_failures(self):
        """Test fallback to basic parsing when all LLMs fail"""
        
        client = LLMClient()
        
        # Mock both LLMs failing
        mock_openai_client = Mock()
        mock_openai_client.chat.completions.create.side_effect = Exception("OpenAI failed")
        client.openai_client = mock_openai_client
        
        mock_gemini_client = Mock()
        mock_gemini_client.generate_content.side_effect = Exception("Gemini failed")
        client.gemini_client = mock_gemini_client
        
        result = await client.parse_analysis_request("Test question", {})
        
        # Should get basic fallback
        assert result["analysis_type"] == "descriptive"
        assert result["data_operations"] == ["clean_data", "basic_stats"]
        assert result["expected_outputs"] == "Basic statistical analysis and visualizations"
    
    @pytest.mark.asyncio
    async def test_parse_with_no_llm_clients(self):
        """Test parsing when no LLM clients are available"""
        
        client = LLMClient()
        client.openai_client = None
        client.gemini_client = None
        
        result = await client.parse_analysis_request("Test question", {})
        
        # Should immediately use basic fallback
        assert result["analysis_type"] == "descriptive"
        assert "basic_stats" in result["data_operations"]


class TestLLMClientCodeGeneration:
    """Test LLMClient code generation functionality"""
    
    @pytest.mark.asyncio
    async def test_code_generation_with_openai(self):
        """Test code generation with OpenAI"""
        
        client = LLMClient()
        
        mock_openai_client = Mock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = """
import pandas as pd
import numpy as np

# Generated analysis code
results = df.describe()
"""
        
        mock_openai_client.chat.completions.create.return_value = mock_response
        client.openai_client = mock_openai_client
        
        analysis_plan = {"analysis_type": "descriptive"}
        data_schema = {"columns": ["A", "B"]}
        
        result = await client.generate_analysis_code(analysis_plan, data_schema)
        
        assert "import pandas as pd" in result
        assert "results = df.describe()" in result
    
    @pytest.mark.asyncio  
    async def test_code_generation_fallback(self):
        """Test code generation fallback when LLMs fail"""
        
        client = LLMClient()
        client.openai_client = None
        client.gemini_client = None
        
        result = await client.generate_analysis_code({}, {})
        
        # Should get basic fallback code
        assert "import pandas as pd" in result
        assert "df.describe()" in result
        assert "df.hist(" in result


class TestLLMClientRealConfiguration:
    """Test with real configuration (requires actual API keys)"""
    
    def test_real_configuration_status(self):
        """Test what's actually configured in your environment"""
        
        print(f"\nüîß Real Configuration Status:")
        print(f"OpenAI Key: {'‚úÖ SET' if settings.openai_api_key else '‚ùå NOT SET'}")
        print(f"Gemini Key: {'‚úÖ SET' if settings.gemini_api_key else '‚ùå NOT SET'}")
        
        client = LLMClient()
        print(f"OpenAI Client: {'‚úÖ READY' if client.openai_client else '‚ùå NONE'}")
        print(f"Gemini Client: {'‚úÖ READY' if client.gemini_client else '‚ùå NONE'}")
        
        # This test always passes, just shows status
        assert True
    
    @pytest.mark.skipif(not settings.openai_api_key, reason="OpenAI API key not configured")
    @pytest.mark.asyncio
    async def test_real_openai_parsing(self):
        """Test with real OpenAI API (only runs if key is configured)"""
        
        client = LLMClient()
        
        result = await client.parse_analysis_request(
            "Analyze sales data and find trends", 
            {"columns": ["date", "sales", "region"]}
        )
        
        # Real API should return structured data
        assert "analysis_type" in result
        assert "data_operations" in result
        print(f"‚úÖ Real OpenAI test passed: {result['analysis_type']}")
    
    @pytest.mark.skipif(not settings.gemini_api_key, reason="Gemini API key not configured")  
    @pytest.mark.asyncio
    async def test_real_gemini_parsing(self):
        """Test with real Gemini API (only runs if key is configured)"""
        
        client = LLMClient()
        
        # Force Gemini by disabling OpenAI temporarily
        original_openai = client.openai_client
        client.openai_client = None
        
        try:
            result = await client.parse_analysis_request(
                "Analyze customer data for insights",
                {"columns": ["customer_id", "purchase_amount"]}
            )
            
            assert "analysis_type" in result
            print(f"‚úÖ Real Gemini test passed: {result['analysis_type']}")
            
        finally:
            client.openai_client = original_openai 